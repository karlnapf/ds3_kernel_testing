{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing and comparing probabilities with kernels - practical\n",
    "\n",
    "Notebook prepared by [Heiko Strathmann](http://herrstrathmann.de/). Feel free to email me any questions you have.\n",
    "\n",
    "In this notebook, we will cover implementation details and applications for three types of kernel hypothesis tests:\n",
    "    \n",
    "* Two-sampling testing using the Maximum Mean Discrepancy (MMD)\n",
    "* Independence testing using the Hilbert-Schmidt Independence Criterion (HSIC)\n",
    "* Goodness-of-fit testing using the Kernel Stein Discrepancy (KSD)\n",
    "\n",
    "On the fly, we will introduce basic concepts of statistical hypothesis testing.\n",
    "\n",
    "The notebook contains a number of `# IMPLEMENT` comments, where you are supposed to insert missing code yourself.\n",
    "We also prepared a solution notebook that contains the code we envisioned.\n",
    "\n",
    "Cells are meant to be executed in order (i.e. cells might depend on previous ones).\n",
    "\n",
    "## Dependencies\n",
    "The notebook was written using Python 3.6, but we made an effort to keep it backwards compatible to older versions down to 2.7 (hopefully).\n",
    "\n",
    "We encourage you to use a `conda` environment so you don't modify your system's Python setup (especially due to the problems that come with install TensorFlow and Shogun).\n",
    "\n",
    "The packages you need are:\n",
    "* A standard scipy setup covers most content: `numpy`, `scipy`, `matplotlib`, and of yource `jupyter`\n",
    "* `tensorflow` for gradient-based optimization of test power on the MNIST-GAN example. Note we recommend to install this using `conda install -c conda-forge tensorflow `\n",
    "* Optional: `scikit-learn` for a cross-validation splitting iterator (can implement yourself or use another lib)\n",
    "* Optional: `seaborn`, `pandas` for convenient visualization in some examples\n",
    "* Optional: `tqdm` for a fancy progress bar\n",
    "* Optional: Shogun if you want to run a faster MMD test in the MNIST-GAN example: `conda install -c conda-forge shogun` (do this first as the package currently uses openblas and not MKL, so some of the other packages need to be re-installed).\n",
    "\n",
    "This is an example how to set this environment up and run the notebook server\n",
    "```\n",
    "conda create --name ds3_kernels  python=3.6\n",
    "source activate ds3_kernels\n",
    "conda install -c conda-forge tensorflow \n",
    "conda install -c conda-forge shogun\n",
    "conda install jupyter matplotlib scikit-learn seaborn pandas tqdm\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "See the Anaconda/TensorFlow/Shogun installation pages for more details.\n",
    "\n",
    "\n",
    "## Additional files\n",
    "All files can be found at https://github.com/karlnapf/ds3_kernel_testing.\n",
    "They include\n",
    "\n",
    "* This notebook and the solution notebook (we encourage you to not look at this before the practical has finished).\n",
    "* **Provided Python modules**: We provide a few external modules that need to be in the `PYTHONPATH` of your notebook server (just put the files in the same folder as this notebook and start the server from this folder).\n",
    "To see whether this worked, try ```import mmd```, and if it doesn't work, make sure that the file is present in one of the paths when you execute `import sys; print(sys.path)`.\n",
    "* **Provided data**: Put those into the same folder as this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case you cannot install tqdm, uncomment this\n",
    "# def tqdm(x):\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-sample testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin with some simple data $X\\sim P_X, Y\\sim P_Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"simple.npz\")\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"]\n",
    "plt.hist(X, alpha=0.5)\n",
    "plt.hist(Y, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: do X and Y come from the same distribution?\n",
    "Or in other words: can we reject the null hypothesis that $P_X=P_Y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The simplest of all test statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by comparing the means of the samples.\n",
    "Clearly, if the distributions are the same, the mean would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_statistic(X,Y, squared=False):\n",
    "    assert X.ndim == Y.ndim == 1\n",
    "    \n",
    "    # IMPLEMENT: compute mean difference of X and Y\n",
    "    result = \n",
    "    \n",
    "    if squared:\n",
    "        result *= result\n",
    "    return result\n",
    "\n",
    "my_statistic = simple_statistic(X,Y)\n",
    "print(\"Mean differencce:\", my_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significant?\n",
    "What would it be if distributions were the same? What if they were different?\n",
    "\n",
    "Let's assume that $P_X=P_Y$ and look at the distribution of the test statistic (null distribution).\n",
    "For that, we can merge the samples and shuffle them, after which they have the same \"joint\" distribution.\n",
    "We can then re-compute the statistic for multiple permutations to get a feeling for how the statistic is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sample_permutation_test(test_statistic, X, Y, num_permutations, prog_bar=True):\n",
    "    assert X.ndim == Y.ndim\n",
    "    \n",
    "    statistics = np.zeros(num_permutations)\n",
    "    \n",
    "    range_ = range(num_permutations)\n",
    "    if prog_bar:\n",
    "        range_ = tqdm(range_)\n",
    "    for i in range_:\n",
    "        # concatenate samples\n",
    "        if X.ndim == 1:\n",
    "            Z = np.hstack((X,Y))\n",
    "        elif X.ndim == 2:\n",
    "            Z = np.vstack((X,Y))\n",
    "            \n",
    "        # IMPLEMENT: permute samples and compute test statistic\n",
    "        my_test_statistic = \n",
    "        statistics[i] = my_test_statistic\n",
    "    return statistics\n",
    "\n",
    "num_permutations = 200\n",
    "statistics = two_sample_permutation_test(simple_statistic, X, Y, num_permutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an idea about what to expect from the statistic in case where the distributions are equal.\n",
    "How likely is our particular value of that statistic?\n",
    "We can compare it against the say 95% quantiles of the null distribution.\n",
    "If our statistic lies outside those, we can reject the null with a confidence of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_permutation_samples(null_samples, statistic=None):\n",
    "    plt.hist(null_samples)\n",
    "    plt.axvline(x=np.percentile(null_samples, 0.25), c='b')\n",
    "    legend = [\"95% quantiles\"]\n",
    "    if statistic is not None:\n",
    "        plt.axvline(x=statistic, c='r')\n",
    "        legend += [\"Actual test statistic\"]\n",
    "    plt.legend(legend)\n",
    "    plt.axvline(x=np.percentile(null_samples, 97.5), c='b')\n",
    "    plt.xlabel(\"Test statistic value\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    \n",
    "plot_permutation_samples(statistics, my_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, depending on the test statistic, there might be multiple ways to compute the null distribution.\n",
    "Not all of them work via sampling it directly, which also can be costly.\n",
    "For example, if we just use the mean difference as a test statistic, and then plug in two Gaussian distributed random variables, we know that the distribution of the statistic is also Gaussian (see the above plot). \n",
    "We might be even able to analytically work out the parameters of this distribution.\n",
    "\n",
    "In case you haven't realized, this is basically a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test) where we assume that we know that both distributions have unit variance and we can distinguish them via solely looking at their mean.\n",
    "As it will be useful for kernel based test statistics later, we will also look at the distribution of a squared test statistic, in order to make it strictly positive (and zero in expectation if both distributions are the same).\n",
    "Naturally, the average of squared Gaussian random variables has a chi-square distribution -- something that can be used for the kernel tests that we introduce later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT: use squared test statistic\n",
    "simple_statistic_squared = \n",
    "\n",
    "# visualise test\n",
    "statistics = two_sample_permutation_test(simple_statistic_squared, X, Y, num_permutations)\n",
    "my_statistic = simple_statistic_squared(X,Y)\n",
    "plot_permutation_samples(statistics, my_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative distribution via explicit simulation\n",
    "\n",
    "As a caveat, please note that the test statistic itself has a distribution under the alternative hypothesis ($P_X \\neq P_Y$), and that our computed test statistic is only a single sample from this distribution.\n",
    "In practice, it is unfortunately not possible to look at the distribution of the test under the alternative, as this would require a way to generate more data.\n",
    "We can try this for a synthetic example where we have infinite data though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# we make a model where the null distribution is false\n",
    "shift = .3\n",
    "X = np.random.randn(N)\n",
    "Y = np.random.randn(N)+shift\n",
    "\n",
    "statistics_null = two_sample_permutation_test(simple_statistic_squared, X, Y, num_permutations)\n",
    "my_statistic = simple_statistic_squared(X,Y) # this is a single sample from the alternative\n",
    "\n",
    "statistics_alt = np.zeros(num_permutations)\n",
    "for i in tqdm(range(num_permutations)):\n",
    "    # IMPLEMENT: generate more data from the alternative\n",
    "    X = \n",
    "    Y = \n",
    "    \n",
    "    statistics_alt[i] = \n",
    "    \n",
    "plt.figure(figsize=(12,4))\n",
    "plot_permutation_samples(statistics_null, my_statistic)    \n",
    "plt.hist(statistics_alt, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even though most of the alternative distribution far larger than the null, we can be unlucky: there are datasets for which the test would not reject the null simply by chance -- we only have a single draw from the alternative.\n",
    "With this fixed seed, it turns out that we have been unlucky and would have not rejected the null hypothesis even it is not true.\n",
    "\n",
    "Let's try the test on another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"almost_simple.npz\")\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"]\n",
    "plt.hist(X, alpha=0.5)\n",
    "plt.hist(Y, alpha=0.5)\n",
    "\n",
    "# IMPLEMENT: print mean and std deviation of data\n",
    "print()\n",
    "\n",
    "my_statistic = simple_statistic(X,Y)\n",
    "statistics = two_sample_permutation_test(simple_statistic, X, Y, num_permutations)\n",
    "\n",
    "plt.figure()\n",
    "plot_permutation_samples(statistics, my_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the squared test statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT: visualize test for squared statistic\n",
    "simple_statistic_squared = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both test statistics essentially have no chance in distinguishing this data.\n",
    "(althogh keep in mind we could have been unlucky as outlined above).\n",
    "For now, let's just assume we only have a single dataset to compute the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic time MMD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen the quadratic time MMD: it was the squared difference of the sample means above, if we had used a linear kernel (where the feature space embedding is the identity function).\n",
    "This test wasn't able to distinguish the above distributions, but let's try a different kernel.\n",
    "\n",
    "See the slides for the definition of the MMD test statistic.\n",
    "We will use the unbiased statistic here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to implement a kernel function.\n",
    "Though we could use a framework to do this for us, it is quite instructive to do this ourselves (at least once).\n",
    "\n",
    "We will start with a Gaussian kernel\n",
    "$$\n",
    "k(x,y) = \\exp \\left( - \\frac{||x-y||_2^2}{\\sigma}\\right)\n",
    "$$\n",
    "with some bandwidth parameter $\\sigma$ (note there are many different parametrizations!).\n",
    "\n",
    "This kernel is translation invariant, and like many other kernels, it is defined in terms of pairwise (Euclidean) distances between the data.\n",
    "Let's implement it this way so we can easily try other kernels that are translation invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform, pdist, cdist\n",
    "\n",
    "def sq_distances(X,Y=None):\n",
    "    assert(X.ndim==2)\n",
    "\n",
    "    # IMPLEMENT: compute pairwise distance matrix. Don't use explicit loops, but the above scipy functions\n",
    "    # if X=Y, use more efficient pdist call which exploits symmetry\n",
    "    if Y is None:\n",
    "        sq_dists = \n",
    "    else:\n",
    "        assert(Y.ndim==2)\n",
    "        assert(X.shape[1]==Y.shape[1])\n",
    "        sq_dists = \n",
    "\n",
    "    return sq_dists\n",
    "\n",
    "def gauss_kernel(X, Y=None, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Computes the standard Gaussian kernel k(x,y)=exp(- ||x-y||**2 / (2 * sigma**2))\n",
    "\n",
    "    X - 2d array, samples on left hand side\n",
    "    Y - 2d array, samples on right hand side, can be None in which case they are replaced by X\n",
    "    \n",
    "    returns: kernel matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # IMPLEMENT: compute squared distances and kernel matrix\n",
    "    sq_dists = \n",
    "    K = \n",
    "    return K\n",
    "\n",
    "# IMPLEMENT\n",
    "def linear_kernel(X, Y):\n",
    "    K =\n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the kernel at hand, we can implement the quadratic time MMD statistic as a function of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_time_mmd(X,Y,kernel):\n",
    "    assert X.ndim == Y.ndim == 2\n",
    "    K_XX = kernel(X,X)\n",
    "    K_XY = kernel(X,Y)\n",
    "    K_YY = kernel(Y,Y)\n",
    "       \n",
    "    n = len(K_XX)\n",
    "    m = len(K_YY)\n",
    "    \n",
    "    # IMPLEMENT: unbiased MMD statistic (could also use biased, doesn't matter if we use permutation tests)\n",
    "    mmd = \n",
    "    return mmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_kernel = lambda X,Y : gauss_kernel(X,Y,sigma=0.3)\n",
    "my_mmd = lambda X,Y : quadratic_time_mmd(X[:,np.newaxis],Y[:,np.newaxis], my_kernel)\n",
    "\n",
    "statistics = two_sample_permutation_test(my_mmd, X, Y, num_permutations)\n",
    "my_statistic = my_mmd(X,Y)\n",
    "\n",
    "plot_permutation_samples(statistics, my_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems much better: the Gaussian kernel allows us to very clearly distinguish the two distributions.\n",
    "\n",
    "Now let us try the MMD with a linear kernel and compare it to the above.\n",
    "In particular, compare (and think about) the relationship to the simple squared mean difference statistic we computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT: visualize test using linear kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost the same result as we had for doing the simple squared statistic above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT (optional): try mix and matching kernels and data sources\n",
    "# e.g. try a periodic kernel on data that differs in frequency space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative methods to estimate the null distribution\n",
    "\n",
    "There are multiple other way to get our hands on the distribution of the MMD test statistic under the null hypothesis.\n",
    "\n",
    "We know that asymptotically, the distribution is an infinite sum of Chi-square variables -- intractable unfortunately.\n",
    "Techniques to approximate it include\n",
    "\n",
    "* moment matching using a Gamma distribution: fast, but doesn't result in a consistent test\n",
    "* a spectral approximation: use Eigenvalues of the kernel matrix to sample from the null distribution in closed form. Costly (cubic!) for large sample sets.\n",
    "* wild-bootstrap: a technique used for correlated samples that is similar to permuting\n",
    "* linear time statistics: for those, one can often show that the null distribution is Gaussian and its variance can be estimated in closed form.\n",
    "\n",
    "We do not cover those here since they require some more details.\n",
    "The permutation test, while potentially slower, is easy to understand and works for most applications (if implemented well).\n",
    "\n",
    "Some resources:\n",
    "* [a Shogun notebook](http://www.shogun-toolbox.org/notebook/latest/mmd_two_sample_testing.html), old but contains many null approximation methods\n",
    "* Linear time tests based on analytic function comparisons (see notes at end of section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD witness function in the RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature of the MMD is that we can see where the density functions are different via querying the (empirical) MMD witness function.\n",
    "This is just the difference of the empirical mean embeddings in the RKHS and thus computable (remember the MMD is the RKHS norm of this witness function).\n",
    "See the slides for details.\n",
    "\n",
    "$$f^*(\\cdot)=\\hat \\mu_X(\\cdot) - \\hat \\mu_Y(\\cdot) $$\n",
    "\n",
    "Let's define a grid and evaluate the witness function on it to see where X and Y differ most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = np.linspace(np.min(X), np.max(X))\n",
    "\n",
    "my_kernel = lambda X,Y : gauss_kernel(X,Y, sigma=0.5)\n",
    "\n",
    "# IMPLEMENT: evaluate MMD witness function on grid\n",
    "phi_X = \n",
    "phi_Y = \n",
    "witness = phi_X-phi_Y\n",
    "\n",
    "plt.plot(grid, witness)\n",
    "\n",
    "plt.hist(X, alpha=0.5, normed=True)\n",
    "plt.hist(Y, alpha=0.5, normed=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the witness function is positive where X as a higher density than Y, and negative vice versa.\n",
    "It is zero where both densities match.\n",
    "Intuitively, the RKHS norm of this function can only be zero if the densities match everywhere, and it grows as the densities differ on more and more points in their support.\n",
    "Of course, this kind of visualization only works in few dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on efficient implementations\n",
    "Note that the above implemention of the MMD, and in particular the permutation test, is extremelty inefficient since the kernel matrix is re-computed in every iteration of the permutation test (and the data is copied around for concatenation as well).\n",
    "We could precompute it once, but then we still would be required to permute it multiple times.\n",
    "\n",
    "We have written a multi-threaded version in C++ that does not permute the matrix, but simply traverses it multiple times, which makes the algorithm L2 cache friendly and therefore hundreds of times faster.\n",
    "See:\n",
    "* http://shogun.ml for the toolbox\n",
    "* http://shogun.ml/examples/latest/examples/statistical_testing/quadratic_time_mmd.html for an API example\n",
    "* http://www.shogun-toolbox.org/notebook/latest/mmd_two_sample_testing.html for a complex demo in Python\n",
    "* https://arxiv.org/abs/1611.04488 for a paper with performance details\n",
    "\n",
    "We will use Shogun for a larger example below.\n",
    "You could time our handwritten implementation against it, or you can tune the Python implementation and try to beat it ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel learning (aka which kernel parameter to choose?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the Gaussian kernel above increased the power of the test.\n",
    "But what happened if we chose a wrong kernel parameter?\n",
    "Let's pick a bad one and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kernel = lambda X,Y : gauss_kernel(X,Y,sigma=0.001)\n",
    "my_mmd = lambda X,Y : quadratic_time_mmd(X[:,np.newaxis],Y[:,np.newaxis], my_kernel)\n",
    "\n",
    "statistics = two_sample_permutation_test(my_mmd, X, Y, num_permutations)\n",
    "my_statistic = my_mmd(X,Y)\n",
    "plot_permutation_samples(statistics, my_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is quite different, the performance of kernel tests indeed crucially depends on the chosen parameters.\n",
    "How to chose those in practice?\n",
    "\n",
    "For simple datasets, there is a heuristic rule to chose the parameter of a Gaussian kernel: using the median of the pairwise distances of the data.\n",
    "See https://arxiv.org/abs/1707.07269 for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_median_heuristic(Z):\n",
    "    # IMPLEMENT: compute the median of the pairwise distances in Z\n",
    "    # (not taking zero distance between identical samples (diagonal) into account)\n",
    "    median_dist = \n",
    "    \n",
    "    return np.sqrt(median_dist/2.0) # our kernel uses a bandwidth of 2*(sigma**2)\n",
    "\n",
    "sigma_median = gaussian_kernel_median_heuristic(np.vstack((X[:,np.newaxis],Y[:,np.newaxis])))\n",
    "print(sigma_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could probably get away with sub-sampling the data to compute pair-wise distances to speed up computing the heuristic, as it stabilizes quickly.\n",
    "In addition, we should probably re-use the pairwise distances when computing the kernel matrix; again, see Shogun.\n",
    "\n",
    "Let's try the heuristic for the statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kernel = lambda X,Y : gauss_kernel(X,Y,sigma=sigma_median)\n",
    "my_mmd = lambda X,Y : quadratic_time_mmd(X[:,np.newaxis],Y[:,np.newaxis], my_kernel)\n",
    "\n",
    "statistics = two_sample_permutation_test(my_mmd, X, Y, num_permutations)\n",
    "my_statistic = my_mmd(X,Y)\n",
    "plot_permutation_samples(statistics, my_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked great!\n",
    "But it doesn't always.\n",
    "\n",
    "Let's try the median heuristic on the following dataset, referred to as Gaussian blobs, see https://papers.nips.cc/paper/4727-optimal-kernel-choice-for-large-scale-two-sample-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"blobs_zoomed.npz\")\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"]\n",
    "plt.plot(X[:,0], X[:,1], 'b.', alpha=0.5)\n",
    "plt.plot(Y[:,0], Y[:,1], 'r.', alpha=0.5)\n",
    "plt.title(\"One blob\")\n",
    "\n",
    "plt.figure()\n",
    "data = np.load(\"blobs.npz\")\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"]\n",
    "plt.plot(X[:,0], X[:,1], 'b.', alpha=0.5)\n",
    "plt.plot(Y[:,0], Y[:,1], 'r.', alpha=0.5);\n",
    "plt.title(\"All blobs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the two distributions are very different.\n",
    "On the grid, the scale at which they differ, however, is somewhat invisible from the median heuristic, which looks at global scaling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigma_median = gaussian_kernel_median_heuristic(np.vstack((X,Y)))\n",
    "print(\"median bw/sigma:\", 2*(sigma_median**2), \"/\", sigma_median)\n",
    "\n",
    "# IMPLEMENT: visualize test using median heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow, the median distance does not really represent the scale on which the two distribution are differing here (they clearly do!).\n",
    "We have figure out how to fix that though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using cross-validation to select a single kernel parameter\n",
    "\n",
    "As a modern ML scientist, your first reflex might be to just try a bunch of different kernel parameters, and compute a cross-validation estimate of the test power.\n",
    "This involves computing the test for the \"training partitions\" of every fold, and average the number of rejections.\n",
    "Let's start with that.\n",
    "Note that we have to use a different \"training\" dataset to learn the kernel -- otherwise the test would over-fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with few values\n",
    "log_sigmas = np.array([-5, 0., 10])\n",
    "\n",
    "num_folds = 5\n",
    "data = np.load(\"blobs2.npz\")\n",
    "X_learn = data[\"X\"]\n",
    "Y_learn = data[\"Y\"]\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "test_powers = np.zeros(len(log_sigmas))\n",
    "np.random.seed(0)\n",
    "\n",
    "for i, log_sigma in tqdm(list(enumerate(log_sigmas))):\n",
    "    # IMPLEMENT: define kernel and mmd\n",
    "    my_kernel = \n",
    "    my_mmd = \n",
    "\n",
    "    rejects = []\n",
    "    for train, _ in  KFold(len(X_learn), n_folds=5, shuffle=True):\n",
    "        # IMPLEMENT: split data into train and test folds\n",
    "        X_learn_train = \n",
    "        Y_learn_train = \n",
    "        \n",
    "        # IMPLEMENT: compute test statistic and null distribution\n",
    "        statistics = # set progress bar to False here\n",
    "        my_statistic = \n",
    "                \n",
    "        # IMPLEMENT: reject null if\n",
    "        p_value = \n",
    "        rejected = # boolean or [0,1]\n",
    "        rejects += [rejected]\n",
    "    \n",
    "    test_powers[i] = np.mean(rejects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(log_sigmas, test_powers)\n",
    "plt.xlabel(\"Log sigma\")\n",
    "plt.ylabel(\"Test power estimate\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to give us an idea of which kernel parameter is better.\n",
    "Unfortunately, this approach is quite costly and noisy.\n",
    "In reality, we would also want to use more folds, repeat the x-validation, and try out more kernel parameters (play with it and you will see).\n",
    "\n",
    "We could try one of the faster approximations for the null distribution mentioned above, for example the moment matching Gamma approach.\n",
    "This would speed up the procedure, however, the Gamma approximation has its limits in how accurate it is, especially for extreme values of the kernel.\n",
    "\n",
    "Instead, we will do something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal kernel via maximising the ratio of mmd and its variance, see https://arxiv.org/abs/1611.04488"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that estimating the test power using cross-validation is a bit messy; it is also not differentiable.\n",
    "\n",
    "Luckily, there is a closed form criterion that acts as a surrogate for the test power: the MMD divided by its variance.\n",
    "You have seen in the slides that maximising this directly maximises the power of the statistical test -- without ever computing the test.\n",
    "\n",
    "Long story short, you can\n",
    "\n",
    "1. Pick a training set (not the one you use for performing the test, to avoid overfitting)\n",
    "2. Choose the kernel that maximises the value of the MMD, divided by its standard variance\n",
    "3. Enjoy a test that has higher power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As implementing the variance of the MMD estimator is slightly more complicated, we have provided a TensorFlow implementation for you (so we can later automagically differentiate it).\n",
    "Feel free to examine the imported code in `mmd.py`.\n",
    "\n",
    "(Note that the provided implementation uses a biased MMD statistic, and as such produces slightly different numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# provided implementation\n",
    "import tensorflow as tf\n",
    "import mmd\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(\"MMD and ratio:\", sess.run(mmd.rbf_mmd2_and_ratio(X,Y, sigma=sigma_median)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the most naive way to select a kernel to to come up with a number of baseline kernels, then simply compute the ratio of MMD and standard deviation for all of them, and finally to pick the kernel with the largest ratio.\n",
    "Remember it is important to do this on a separate \"training\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"blobs2.npz\")\n",
    "X_learn = data[\"X\"]\n",
    "Y_learn = data[\"Y\"]\n",
    "\n",
    "# sigma = 10**log_sigma\n",
    "log_sigmas = np.linspace(-2, 2,10)\n",
    "                         \n",
    "ratios = np.empty(len(log_sigmas))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # IMPLEMENT: compute ratio for every sigma\n",
    "    for i, log_sigma in tqdm(list(enumerate(log_sigmas))):\n",
    "        current_ratio = \n",
    "        ratios[i] = current_ratio\n",
    "\n",
    "plt.plot(log_sigmas, ratios)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Log sigma\")\n",
    "plt.ylabel(\"Ratio of MMD and variance (proportional to test power)\")\n",
    "\n",
    "print(\"best sigma:\", 10**log_sigmas[np.argmax(ratios)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was much faster, even for more parameters to try.\n",
    "\n",
    "The curve looks very much like those obtained when cross-validating kernel-parameters for kernel machines (like a support vector machine): you make the model/test more and more complex (bandwidth smaller), until it hits a sweet spot (peak), and after which it starts overfitting (ratio decreasing).\n",
    "Let's try out how it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_opt = 0.215\n",
    "\n",
    "# IMPLEMENT: visualise test for best sigma\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Victory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "Now, what if we have more than one kernel parameter?\n",
    "Trying out all the different combinations is not the best idea.\n",
    "Instead, we can take a deep learning artisan's mindset and do gradient descent.\n",
    "That is, we maximize the ratio of MMD to standard deviation as a function of the kernel parameters (differentiable!), we could even do this in mini-batches.\n",
    "\n",
    "The implementation of the ratio is quite tricky (you might have checked the imported code).\n",
    "Differentiating it would be a nightmare, but luckily in 2018 we have tools at hand that allow us to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANs\n",
    "\n",
    "Moving to the context of training GANs (Generative adversarial networks).\n",
    "Assume someone came to you with a new GAN, which generates some realistic looking images, supposedly indistinguishable from the images used to train the GAN.\n",
    "\n",
    "Let's assume that person said (https://arxiv.org/abs/1606.03498)\n",
    "> On MTurk, annotators were able to distinguish\n",
    "samples in 52.4% of cases (2000 votes total), where 50% would be obtained by random\n",
    "guessing. Similarly, researchers in our institution were not able to find any artifacts that would allow\n",
    "them to distinguish samples.\n",
    "\n",
    "Let's see what the kernel MMD thinks about this.\n",
    "We will use a slightly different kernel than before: an ARD kernel.\n",
    "This one that scales each of the $D$ input dimensions by a parameter, and then uses a standard Gaussian kernel (unit bandwidth) on top of that.\n",
    "$$ k(x,y) = \\exp\\left(-\\sum_d^D (x_d/\\sigma_d - y_d/\\sigma_d)^2\\right) $$\n",
    "Again, we have implemented this for you in `mmd.py`, feel free to see how.\n",
    "\n",
    "We begin with visualizing the data, and splitting it into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = np.load(\"samples-to-compare.npz\")[\"target\"]\n",
    "# before fitting the model, we binarized the images\n",
    "X[X<0.5]=0\n",
    "X[X>=0.5]=1\n",
    "\n",
    "Y = np.load(\"samples-to-compare.npz\")[\"model\"]\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.suptitle(\"Train samples\")\n",
    "\n",
    "for i in np.arange(0,16):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(X[i],cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.suptitle(\"GAN samples\")\n",
    "for i in np.arange(0,16):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(Y[i],cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "# split off a training set, reshape to vectors\n",
    "N = len(X)\n",
    "assert len(X) == len(Y)\n",
    "size = X.shape[1]\n",
    "size2 = size**2\n",
    "assert size2 == X.shape[2]**2\n",
    "half = int(N/2)\n",
    "X_learn = X[half:].reshape(half,size2)\n",
    "Y_learn = Y[half:].reshape(half,size2)\n",
    "X = X[:half].reshape(half,size2)\n",
    "Y = Y[:half].reshape(half,size2)\n",
    "\n",
    "print(\"Image dimensions: %dx%d=%d\" % (size, size, size2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some code to compute the ratio we want to optimize, and make TensorFlow's Adam optimizer do the rest for us, i.e optimizing the bandwidth variables, $28^2$ of them.\n",
    "\n",
    "Note: this is a very simplistic implementation that might suffer from all sorts of problems.\n",
    "In practice, we would pay more careful attention to the optimization scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigmas = tf.Variable(np.ones(X_learn.shape[1]), dtype=np.float32, name=\"sigmas\")\n",
    "\n",
    "# provided function. note we are using the \"training\" dataset here.\n",
    "ratio = mmd.ard_mmd2_and_ratio(X_learn,Y_learn, bws=sigmas)[1]\n",
    "\n",
    "# IMPLEMENT: set up adam optimizer to maximise the ratio\n",
    "learning_rate = 0.5\n",
    "optimizer = \n",
    "minimize = # tf_op minimization step\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    \n",
    "    print(\"start,\", \"ratio:\", sess.run(ratio))\n",
    "    \n",
    "    for step in range(20):  \n",
    "        # IMPLEMENT: run optimizer and update best found sigmas so far\n",
    "        \n",
    "        print(\"step,\", \"ratio:\", sess.run(ratio))\n",
    "        best_sigmas = \n",
    "    print(\"done, ratio:\", sess.run(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now have learnt one bandwidth per pixel, we can visualise the scaled data to see what the resulting test focusses on.\n",
    "Note that we do this on the actual test data, while we learned the weights on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT: scale images according to sigmas\n",
    "X_scaled = \n",
    "Y_scaled = \n",
    "\n",
    "plt.suptitle(\"Train samples\")\n",
    "plt.figure(figsize=(12,3))\n",
    "for i in np.arange(0,16):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(X_scaled[i].reshape(size, size),cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.suptitle(\"GAN samples\")\n",
    "for i in np.arange(0,16):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(Y_scaled[i].reshape(size, size),cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.figure()\n",
    "plt.title(\"ARD weights\")\n",
    "plt.imshow(best_sigmas.reshape(size,size), 'magma')\n",
    "plt.colorbar()\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there seem to be pixel level artifacts that the test focuses on.\n",
    "As it turns out, focusing on those allows the test to fully distinguish training samples from GAN samples.\n",
    "\n",
    "Since our above implementation for the MMD permutation test is very inefficient, we don't use it here to perform the full test -- it would take too long.\n",
    "You can use the below Shogun snippet to perform the test much more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shogun as sg\n",
    "sg.get_global_parallel().set_num_threads(4)\n",
    "\n",
    "sigma = 1\n",
    "kernel_width = 2 * sigma**2\n",
    "mmd = sg.QuadraticTimeMMD()\n",
    "mmd.set_p(sg.RealFeatures(X_scaled.T.astype(np.float64))) # shogun uses column major format\n",
    "mmd.set_q(sg.RealFeatures(Y_scaled.T.astype(np.float64)))\n",
    "mmd.set_kernel(sg.GaussianKernel(10, kernel_width))\n",
    "\n",
    "mmd.set_num_null_samples(num_permutations)\n",
    "statistics = mmd.sample_null()\n",
    "my_statistic = mmd.compute_statistic()\n",
    "plot_permutation_samples(statistics, my_statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT (optional): how does this compare to all weights being one.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the unscaled features already allow the test to distinguish the distribution quite well.\n",
    "The scaled version above, however, is a much more powerful test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Have much more data? Try the linear time versions of the tests\n",
    "* https://arxiv.org/abs/1605.06796\n",
    "* https://arxiv.org/abs/1506.04725\n",
    "\n",
    "\n",
    "Need faster implementations (of the quadratic time MMD tests)\n",
    "* use Shogun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to independence testing.\n",
    "Here, the null hypothesis is that the two sets of samples come from independent distributions: $P_{XY} = P_X\\cdot P_Y$.\n",
    "\n",
    "We will use the HSIC, which you can think of as the MMD between the joint distribution $P_XY$ and the product of the marginal distributions $P_X P_Y$. \n",
    "Clearly, if those are different, then the samples cannot be independent.\n",
    "\n",
    "See the slides for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic time test statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kick of with implementing the quadratic time version of the HSIC statistic.\n",
    "\n",
    "$$ \\frac{1}{n^2}\\text{Trace}(KL) $$\n",
    "\n",
    "where $K$ and $L$ are centered kernel matrices of the samples from the two distribution respectively.\n",
    "(Note that one can center a kernel matrix via multiplying it with a centering matrix $H=I-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$.)\n",
    "\n",
    "As our kernel will quite expensive to be computed, we here write a version that accepts precomputed kernel matrices rather than a kernel function, and we will permute this kernel matrix for the permutation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_joint_kernel_matrix(K_XX, K_YY, K_XY):\n",
    "    \"\"\"\n",
    "    Builds the joint XY kernel matrix [[K_XX, K_XY], [K_XY.T, K_YY]],\n",
    "    permutes it, and returns the 4 main blocks (i.e. the joint kernel matrix\n",
    "    if X and Y were merged and permuted.)\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(K_XX)\n",
    "    assert N == len(K_YY)\n",
    "    assert N == K_XY.shape[0]\n",
    "    assert N == K_XY.shape[1]\n",
    "    \n",
    "    # joint X-Y kernel matrix\n",
    "    K_Z = np.zeros((N*2,N*2))\n",
    "    \n",
    "    # IMPLEMENT: populate joint matrix and permute\n",
    "    \n",
    "    K_XX_ = # upper left block in permuted XY matrix\n",
    "    K_YY_ = \n",
    "    K_XY_ = \n",
    "    \n",
    "    # return blocks of permuted matrix: upper left, lower right, and upper right\n",
    "    return K_XX_, K_YY_, K_XY_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the test statistic as a function of kernel matrices, and with an option to permute the matrices before computing the statistic (for approximating the null distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HSIC(K_XX, K_YY, K_XY=None, permute=False):\n",
    "    assert len(K_XX) == len(K_YY)\n",
    "    N = len(K_XX)\n",
    "    if permute:\n",
    "        # we only need K_KXY if we permute the kernel matrices\n",
    "        assert not K_XY is None\n",
    "        K_XX_, K_YY_, K_XY_ = permute_joint_kernel_matrix(K_XX, K_YY, K_XY)\n",
    "    else:\n",
    "        K_XX_ = K_XX\n",
    "        K_YY_ = K_YY\n",
    "\n",
    "    # IMPLEMENT: HSIC statistic\n",
    "    H = np.eye(N) - 1.0/N\n",
    "    statistic = \n",
    "    return statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"hsic2.npz\")\n",
    "X = data[\"X\"]\n",
    "Y = data[\"Y\"]\n",
    "plt.plot(X,Y,'.')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is some very simple dependence going on.\n",
    "The first thing a statistician in the field would do is of course to check for correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT:compute the correlation between X and Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is very low.\n",
    "A kernel test for independence is more powerful, even with a bandwidth parameter chosen by the median heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT: compute kernel matrices using the median heuristic\n",
    "sigma_median = \n",
    "my_kernel = \n",
    "K_XX = my_kernel(X,X)\n",
    "K_YY = my_kernel(Y,Y)\n",
    "K_XY = my_kernel(X,Y)\n",
    "\n",
    "statistics = np.empty(num_permutations)\n",
    "for i in tqdm(range(num_permutations)):\n",
    "    # IMPLEMENT: compute HSIC statistic with permuted matrices\n",
    "    statistics[i] = \n",
    "    \n",
    "# IMPLEMENT: visualise test\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Victory once again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## European parliament documents translations and string kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do a slightly more elaborate test that involves some very mild NLP.\n",
    "More precisely we will analyze dependence between documents.\n",
    "We will use transcripts of the Canadian parliament's house debates, downloaded from [here](https://www.isi.edu/natural-language/download/hansard/).\n",
    "Those consist of pairs of French and English transcripts.\n",
    "\n",
    "Our question here is whether we can detect this supposedly strong dependence structure using the kernel HSIC.\n",
    "Note that this approach does not rely on attempting to translate the documents, but rather on comparing within-document structure. \n",
    "HSIC compares the self-similarity within the English documents with self-similarity of the French ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step, we load the data, and store the transcript of each session into lists X and Y, for English and French respectively.\n",
    "We don't do any sophisticated preprocessing here, but feel free to make this more elaborate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download english and french stopwords (optional)\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords');\n",
    "    stop_english = set(stopwords.words(\"english\"))\n",
    "    stop_french = set(stopwords.words(\"french\"))\n",
    "except Exception:\n",
    "    stop_english = set()\n",
    "    stop_french = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for fname in tqdm(sorted(list(glob(\"transcripts/hansard.36.1.house.debates.*.*\")))):\n",
    "    is_french = fname.endswith(\"f\")\n",
    "    is_english = fname.endswith(\"e\")\n",
    "    \n",
    "    if not (is_english or is_french):\n",
    "        continue\n",
    "        \n",
    "    with open(fname, \"r\", encoding='latin-1') as f:\n",
    "        # some very very simple preprocessing, could be more elaborate with special characters\n",
    "        content = \"\".join(f.readlines())\n",
    "        if is_english:\n",
    "            letters_only = re.sub(\"[^a-zA-Z]\", \" \", content)\n",
    "        elif is_french:\n",
    "            letters_only = re.sub(\"[^a-zA-Z\\'']\", \" \", content) \n",
    "        words = letters_only.lower().split()\n",
    "        \n",
    "        if is_english:\n",
    "            stops = stop_english\n",
    "        elif is_french:\n",
    "            stops = stop_french\n",
    "        meaningful_words = [w for w in words if not w in stops]\n",
    "        \n",
    "        result = \" \".join(meaningful_words)\n",
    "        \n",
    "        if fname.endswith(\"e\"):\n",
    "            X += [result]\n",
    "        else:\n",
    "            Y += [result]\n",
    "N = len(X)\n",
    "assert N == len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some excerpts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X[0][-100:])\n",
    "print(Y[0][-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to construct a string kernel, a  \"bag of words\" kernel between documents $s$ and $t$,\n",
    "\n",
    "$$\n",
    "k(s,t) = \\frac{1}{|\\mathcal{W}|} \\phi(s)^\\top \\phi(t)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{W}$ consists of all words on all considered documents, and each element of $\\phi(x)\\in\\mathbb{N}^{|\\mathcal{W}|}$ contains the number of times that a particular word $w\\in\\mathcal{W}$ appears in $x$.\n",
    "\n",
    "Naturally, the kernel value will be larger, if a word appears in both documents many times.\n",
    "\n",
    "Efficient implementations of string kernels are rare (Shogun has many!), and often based on low-level dynamic programming concepts.\n",
    "Instead, we will here explicitly embed the documents into a feature space and compute (gram) matrix of inner products manually. \n",
    "Note that most string kernel implementations are much more efficient in computing the kernel since they don't do the feature space mapping explicitly.\n",
    "\n",
    "Let's start by building a document embedding mechanism using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             decode_error = 'replace',\n",
    "                             tokenizer = None,    \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 50000)\n",
    "vectorizer.fit(X+Y),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at associated words for each feature space dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "np.random.seed(0)\n",
    "list(feature_names[np.random.permutation(len(feature_names))[-10:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the bag of words kernel, as a product of embedded documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_kernel(fitted_vectoriser, X,Y):\n",
    "    \"\"\"\n",
    "    Computes the gram matrix between the features of X and Y, where the features are computed\n",
    "    using the provided (fitted) sklearn transformer.\n",
    "    \n",
    "    The kernel is k(x,y) = 1/dim(x) * \\phi(x) \\cdot \\phi(y)\n",
    "    \"\"\"\n",
    "    # IMPLEMENT: transform X and Y documents\n",
    "    X_feats = \n",
    "    if X is Y :\n",
    "        Y_feats = X_feats\n",
    "    else:\n",
    "        Y_feats = \n",
    "    \n",
    "    # IMPLEMENT: kernel matrix: inner products between feature vectors\n",
    "    K = \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the kernel matrices (takes some time to compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT: compute kernel matrices between feature vectors using the bag of words kernel\n",
    "K_XX = \n",
    "K_YY = \n",
    "K_XY = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.imshow(K_XX)\n",
    "plt.subplot(132)\n",
    "plt.imshow(K_YY)\n",
    "plt.subplot(133)\n",
    "plt.imshow(K_XY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of shared (self-similarity) structure between X and Y.\n",
    "The HSIC test quantifies this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT: visualise HSIC test for given kernel matrices\n",
    "statistics = np.empty(num_permutations)\n",
    "for i in tqdm(range(num_permutations)):\n",
    "    statistics[i] = \n",
    "    \n",
    "my_statistic = \n",
    "plot_permutation_samples(statistics, my_statistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clear reject of the null hypothesis that X and Y are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "As for the MMD, there are various linear time tests available for the HSIC, in particular https://arxiv.org/abs/1610.04782."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodness of fit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final topic is testing whether a given set of samples comes from a specified distribution.\n",
    "This problem is slightly different from two-sample and independence testing as we are only given a single set of samples, and the reference distribution will be only available through the gradient of the log density (we cannot sample from it).\n",
    "\n",
    "But there are also similarities, i.e. the test statistic is the RKHS norm of a witness function, we can evaluate the witness function, there is a simple kernel-based quadratic time statistic, etc.\n",
    "\n",
    "Unfortunately, as we cannot sample from the reference distribution, we cannot permute our samples in any way to approximate the null distribution.\n",
    "We will use a slightly more sophisticated (still sampling based) way, called the wild bootstrap.\n",
    "This method was originally developed to approximate null distributions for two-sample tests of correlated samples, however, it turns out that we can also utilise the technique for cases where we cannot permute samples.\n",
    "\n",
    "We start with a simple example where we provide you with code to compute test statistic and p-value.\n",
    "The reest of his section is intentionally kept more sparse, encouraging faster participants to implement more advanced concepts on their own.\n",
    "\n",
    "As usual, see the slides for details.\n",
    "\n",
    "The two main references are\n",
    " * https://arxiv.org/abs/1602.02964\n",
    " * https://arxiv.org/abs/1705.07673 (NIPS 2017 best paper award)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing samples of a student-t against a Gaussian\n",
    "\n",
    "We will begin with a simple synthetic example: given a number of samples from a student-t distribution, can we reject the null hypothesis that those samples came from a Gaussian.\n",
    "How does the test change as we make the student-t look more and more like a Gaussian?\n",
    "\n",
    "Remember, we will only use the gradient of the log-density of the reference Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_normal(x):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the log Gaussian density (zero mean, std-dev)\n",
    "    \"\"\"\n",
    "    \n",
    "    # IMPLEMENT:\n",
    "    grad =\n",
    "    \n",
    "    return  grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions of p-values\n",
    "\n",
    "In the beginning of this notebook, we saw that every time a statistical test is computed, we only receive a single test statistic.\n",
    "This is a single sample from the alternative distribution, and leads to a single p-value  (remember the example where we explicitly sampled from the alternative distribution?).\n",
    "\n",
    "Again, this number clearly depends on the data used for the test, a different set of test data will lead to a different test statistic and p-value (while the estimated null distribution should be stable).\n",
    "Under the null hypothesis of any test, the distribution of p-values is uniform, since we just take a sample from the very same distribution and then compute its quantile.\n",
    "Long story short: we never know whether we just have been \"lucky\" when computing the test.\n",
    "\n",
    "In order to get a feeling for this effect, we will now run the goodness-of-fit test multiple times and plot the distribution over p-values.\n",
    "We will perform this experiment using samples from a [student-t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution), and test the null hypothesis that these samples are distributed as a standard Gaussian.\n",
    "As the degrees of freedom are increasing, the samples look more and more Gaussian, and we will quantify the distribution of p-values along the way.\n",
    "\n",
    "Since we need to compute the test many times to sample the distribution of p-values, the quadratic time statistic would take a long time.\n",
    "This is why we have provided an implementation of a linear-time goodness-of-fit test using the kernel Stein discrepancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from ksd import GaussianSteinTest\n",
    "\n",
    "# parameter for the linear time test (see paper if interested)\n",
    "num_random_freq=2\n",
    "\n",
    "N = 500\n",
    "\n",
    "degrees_of_freedom = range(1, 10, 2)\n",
    "num_repetitions = 200\n",
    "results = []\n",
    "\n",
    "for dof, mc in tqdm(list(itertools.product(degrees_of_freedom, range(num_repetitions)))):\n",
    "    # IMPLEMENT: sample from student t with given degrees of freedom (dof)\n",
    "    X = \n",
    "    \n",
    "    # this is how we run the linear time goodness-of-fit test\n",
    "    pvalue = GaussianSteinTest(grad_log_normal,num_random_freq).compute_pvalue(X)\n",
    "    results += [[dof, pvalue]]\n",
    "\n",
    "for mc in tqdm(range(num_repetitions)):\n",
    "        # IMPLEMENT: run the test to sample from the null distribution\n",
    "        X = \n",
    "        pvalue = \n",
    "        \n",
    "        results += [[np.Inf, pvalue]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=[\"dof\",\"p_value\"])\n",
    "sns.boxplot(y=\"p_value\",x=\"dof\",data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for a low number of degrees of freedom, the p-values tend to be mostly small (i.e. the test rejects the null hypothesis more often).\n",
    "As we make the distribution look more like a Gaussian, the p-values increase.\n",
    "Eventually, the student-t becomes a Gaussian for an infinite number of degrees of freedom, i.e. we are sampling from the (unitform!) distribution of p-values under the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic time test statistic\n",
    "\n",
    "Let's implement the quadratic time statistic to get a feeling for what quantities are needed: the gradient of the target log-density, the kernel, and the gradient of the kernel with respect to left and right arguments (individually and both).\n",
    "We will use a standard Gaussian kernel with unit scaling, for simplicity in 1D.\n",
    "The first step is to implement kernel gradients (we could do this using an autodiff framework!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k( x, y):\n",
    "    return np.exp(-np.dot(x - y,x - y))\n",
    "\n",
    "def g1k(x, y):\n",
    "    # IMPLEMENT: gradient wrt left argument\n",
    "    return ...\n",
    "\n",
    "def g2k(x, y):\n",
    "    # IMPLEMENT: gradient wrt right argument\n",
    "    return ...\n",
    "\n",
    "def gk(x, y):\n",
    "    # IMPLEMENT: gradient wrt to both arguments\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ksd import GaussianQuadraticTest\n",
    "\n",
    "X = np.random.standard_t(1,N)\n",
    "kernel_bw = 1.0\n",
    "# reference implementation: the function returns N*statistic. Nevermind the typo :)\n",
    "print(\"Reference\", GaussianQuadraticTest(grad_log_normal, scaling=kernel_bw).get_statistic_multiple(X)[1]/N)\n",
    "\n",
    "# all elements that the statistic sums over\n",
    "U_matrix = np.zeros((N, N))\n",
    "\n",
    "for i in tqdm(range(N)):\n",
    "    for j in range(N):\n",
    "        # IMPLEMENT: implement the quadratic time KSD test statistic (needs gradient of log-density and the gradient of the Gaussian kernel)\n",
    "        # hint: use four terms (see paper, slides)\n",
    "        a = \n",
    "        b = \n",
    "        c = \n",
    "        d = \n",
    "        U_matrix[i, j] = a + b + c + d\n",
    "    \n",
    "stat = np.mean(U_matrix)\n",
    "print(\"My implementation:\", stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, try to visualize the witness function of the kernel Stein discrepancy, just as we visualized the witness function of the MMD above (see slides for details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced exercise\n",
    "\n",
    "Let's see if we can quantify convergence of an MCMC chain using the goodness-of-fit test.\n",
    "For simplification, we will thin the MCMC samples so that they are not correlated (see the original KSD paper on how to run the test on correlated samples).\n",
    "This experiment loosely follows one in the KSD paper.\n",
    "\n",
    "In this open-ended las task you will:\n",
    "\n",
    "* Run an MCMC chain using stochastic gradient Langevin dynamics with a large step-size. See https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf\n",
    "* Target a standard Gaussian distribution in 1D (or 2D if you like)\n",
    "* After the sampler is completed, thin the MCMC samples so that they are almost i.i.d.\n",
    "* Run the goodness-of-fit test to quantify at what step-size of the sampler the samples reach a good quality.\n",
    "\n",
    "Good luck :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
